{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63384c33-081b-45e6-8b76-0f8df77d6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch 模型 保存完整模型，triton推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4152ae-fcbd-4733-a267-205bcd43c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://mirrors.tencent.com/ubuntu bionic InRelease\n",
      "Hit:2 http://mirrors.tencent.com/ubuntu bionic-security InRelease              \u001b[0m\n",
      "Hit:3 http://mirrors.tencent.com/ubuntu bionic-updates InRelease               \u001b[0m\n",
      "Hit:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease  \u001b[0m    \u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Hit:5 https://deb.nodesource.com/node_16.x bionic InRelease                 \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\u001b[0m                \u001b[33m\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "102 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 102 not upgraded.\n",
      "Need to get 168 kB of archives.\n",
      "After this operation, 567 kB of additional disk space will be used.\n",
      "Get:1 http://mirrors.tencent.com/ubuntu bionic-security/main amd64 unzip amd64 6.0-21ubuntu1.1 [168 kB]\n",
      "Fetched 168 kB in 0s (534 kB/s)[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package unzip.\n",
      "(Reading database ... 25422 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-21ubuntu1.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking unzip (6.0-21ubuntu1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up unzip (6.0-21ubuntu1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JLooking in indexes: https://mirrors.tencent.com/pypi/simple/, https://mirrors.tencent.com/repository/pypi/tencent_pypi/simple\n",
      "Collecting torchvision\n",
      "  Downloading https://mirrors.tencent.com/pypi/packages/18/2c/aa3f3193ea406aac402d9fcd30b07246cac096cf8e62d31d43c209b4adfd/torchvision-0.13.0-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
      "     |████████████████████████████████| 19.1 MB 4.3 MB/s            \n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading https://mirrors.tencent.com/pypi/packages/86/c3/30eb447a38bb73d57883ec0941e213249b2001d78332a3026351e0ee8d1f/torch-1.12.0-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
      "     |████████████████████████████████| 776.3 MB 9.8 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.1)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading https://mirrors.tencent.com/pypi/packages/20/cb/261342854f01ff18281e97ec8e6a7ce3beaf8e1091d1cebd52776049358d/Pillow-9.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 990 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2021.10.8)\n",
      "Installing collected packages: torch, pillow, torchvision\n",
      "Successfully installed pillow-9.2.0 torch-1.12.0 torchvision-0.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! apt update -y\n",
    "! apt install -y wget unzip\n",
    "! pip install torchvision torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce28849-4c67-4af7-a8fc-904c181d9194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-30 12:23:36--  https://docker-76009.sz.gfp.tencent-cloud.com/github/cube-studio/inference/resnet50.pth\n",
      "Resolving docker-76009.sz.gfp.tencent-cloud.com (docker-76009.sz.gfp.tencent-cloud.com)... 43.137.222.31, 175.27.38.240, 43.137.221.26, ...\n",
      "Connecting to docker-76009.sz.gfp.tencent-cloud.com (docker-76009.sz.gfp.tencent-cloud.com)|43.137.222.31|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 102530333 (98M) [application/octet-stream]\n",
      "Saving to: ‘resnet50.pth’\n",
      "\n",
      "resnet50.pth        100%[===================>]  97.78M  6.42MB/s    in 15s     \n",
      "\n",
      "2022-07-30 12:23:52 (6.54 MB/s) - ‘resnet50.pth’ saved [102530333/102530333]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# triton 直接推理 torch模型\n",
    "! rm -rf resnet50.pth && wget https://docker-76009.sz.gfp.tencent-cloud.com/github/cube-studio/inference/resnet50.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a853810-b2c7-49f6-a36e-713121133ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n",
      "conv1.weight : torch.Size([64, 3, 7, 7])\n",
      "bn1.weight : torch.Size([64])\n",
      "bn1.bias : torch.Size([64])\n",
      "layer1.0.conv1.weight : torch.Size([64, 64, 1, 1])\n",
      "layer1.0.bn1.weight : torch.Size([64])\n",
      "layer1.0.bn1.bias : torch.Size([64])\n",
      "layer1.0.conv2.weight : torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight : torch.Size([64])\n",
      "layer1.0.bn2.bias : torch.Size([64])\n",
      "layer1.0.conv3.weight : torch.Size([256, 64, 1, 1])\n",
      "layer1.0.bn3.weight : torch.Size([256])\n",
      "layer1.0.bn3.bias : torch.Size([256])\n",
      "layer1.0.downsample.0.weight : torch.Size([256, 64, 1, 1])\n",
      "layer1.0.downsample.1.weight : torch.Size([256])\n",
      "layer1.0.downsample.1.bias : torch.Size([256])\n",
      "layer1.1.conv1.weight : torch.Size([64, 256, 1, 1])\n",
      "layer1.1.bn1.weight : torch.Size([64])\n",
      "layer1.1.bn1.bias : torch.Size([64])\n",
      "layer1.1.conv2.weight : torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight : torch.Size([64])\n",
      "layer1.1.bn2.bias : torch.Size([64])\n",
      "layer1.1.conv3.weight : torch.Size([256, 64, 1, 1])\n",
      "layer1.1.bn3.weight : torch.Size([256])\n",
      "layer1.1.bn3.bias : torch.Size([256])\n",
      "layer1.2.conv1.weight : torch.Size([64, 256, 1, 1])\n",
      "layer1.2.bn1.weight : torch.Size([64])\n",
      "layer1.2.bn1.bias : torch.Size([64])\n",
      "layer1.2.conv2.weight : torch.Size([64, 64, 3, 3])\n",
      "layer1.2.bn2.weight : torch.Size([64])\n",
      "layer1.2.bn2.bias : torch.Size([64])\n",
      "layer1.2.conv3.weight : torch.Size([256, 64, 1, 1])\n",
      "layer1.2.bn3.weight : torch.Size([256])\n",
      "layer1.2.bn3.bias : torch.Size([256])\n",
      "layer2.0.conv1.weight : torch.Size([128, 256, 1, 1])\n",
      "layer2.0.bn1.weight : torch.Size([128])\n",
      "layer2.0.bn1.bias : torch.Size([128])\n",
      "layer2.0.conv2.weight : torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight : torch.Size([128])\n",
      "layer2.0.bn2.bias : torch.Size([128])\n",
      "layer2.0.conv3.weight : torch.Size([512, 128, 1, 1])\n",
      "layer2.0.bn3.weight : torch.Size([512])\n",
      "layer2.0.bn3.bias : torch.Size([512])\n",
      "layer2.0.downsample.0.weight : torch.Size([512, 256, 1, 1])\n",
      "layer2.0.downsample.1.weight : torch.Size([512])\n",
      "layer2.0.downsample.1.bias : torch.Size([512])\n",
      "layer2.1.conv1.weight : torch.Size([128, 512, 1, 1])\n",
      "layer2.1.bn1.weight : torch.Size([128])\n",
      "layer2.1.bn1.bias : torch.Size([128])\n",
      "layer2.1.conv2.weight : torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight : torch.Size([128])\n",
      "layer2.1.bn2.bias : torch.Size([128])\n",
      "layer2.1.conv3.weight : torch.Size([512, 128, 1, 1])\n",
      "layer2.1.bn3.weight : torch.Size([512])\n",
      "layer2.1.bn3.bias : torch.Size([512])\n",
      "layer2.2.conv1.weight : torch.Size([128, 512, 1, 1])\n",
      "layer2.2.bn1.weight : torch.Size([128])\n",
      "layer2.2.bn1.bias : torch.Size([128])\n",
      "layer2.2.conv2.weight : torch.Size([128, 128, 3, 3])\n",
      "layer2.2.bn2.weight : torch.Size([128])\n",
      "layer2.2.bn2.bias : torch.Size([128])\n",
      "layer2.2.conv3.weight : torch.Size([512, 128, 1, 1])\n",
      "layer2.2.bn3.weight : torch.Size([512])\n",
      "layer2.2.bn3.bias : torch.Size([512])\n",
      "layer2.3.conv1.weight : torch.Size([128, 512, 1, 1])\n",
      "layer2.3.bn1.weight : torch.Size([128])\n",
      "layer2.3.bn1.bias : torch.Size([128])\n",
      "layer2.3.conv2.weight : torch.Size([128, 128, 3, 3])\n",
      "layer2.3.bn2.weight : torch.Size([128])\n",
      "layer2.3.bn2.bias : torch.Size([128])\n",
      "layer2.3.conv3.weight : torch.Size([512, 128, 1, 1])\n",
      "layer2.3.bn3.weight : torch.Size([512])\n",
      "layer2.3.bn3.bias : torch.Size([512])\n",
      "layer3.0.conv1.weight : torch.Size([256, 512, 1, 1])\n",
      "layer3.0.bn1.weight : torch.Size([256])\n",
      "layer3.0.bn1.bias : torch.Size([256])\n",
      "layer3.0.conv2.weight : torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight : torch.Size([256])\n",
      "layer3.0.bn2.bias : torch.Size([256])\n",
      "layer3.0.conv3.weight : torch.Size([1024, 256, 1, 1])\n",
      "layer3.0.bn3.weight : torch.Size([1024])\n",
      "layer3.0.bn3.bias : torch.Size([1024])\n",
      "layer3.0.downsample.0.weight : torch.Size([1024, 512, 1, 1])\n",
      "layer3.0.downsample.1.weight : torch.Size([1024])\n",
      "layer3.0.downsample.1.bias : torch.Size([1024])\n",
      "layer3.1.conv1.weight : torch.Size([256, 1024, 1, 1])\n",
      "layer3.1.bn1.weight : torch.Size([256])\n",
      "layer3.1.bn1.bias : torch.Size([256])\n",
      "layer3.1.conv2.weight : torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight : torch.Size([256])\n",
      "layer3.1.bn2.bias : torch.Size([256])\n",
      "layer3.1.conv3.weight : torch.Size([1024, 256, 1, 1])\n",
      "layer3.1.bn3.weight : torch.Size([1024])\n",
      "layer3.1.bn3.bias : torch.Size([1024])\n",
      "layer3.2.conv1.weight : torch.Size([256, 1024, 1, 1])\n",
      "layer3.2.bn1.weight : torch.Size([256])\n",
      "layer3.2.bn1.bias : torch.Size([256])\n",
      "layer3.2.conv2.weight : torch.Size([256, 256, 3, 3])\n",
      "layer3.2.bn2.weight : torch.Size([256])\n",
      "layer3.2.bn2.bias : torch.Size([256])\n",
      "layer3.2.conv3.weight : torch.Size([1024, 256, 1, 1])\n",
      "layer3.2.bn3.weight : torch.Size([1024])\n",
      "layer3.2.bn3.bias : torch.Size([1024])\n",
      "layer3.3.conv1.weight : torch.Size([256, 1024, 1, 1])\n",
      "layer3.3.bn1.weight : torch.Size([256])\n",
      "layer3.3.bn1.bias : torch.Size([256])\n",
      "layer3.3.conv2.weight : torch.Size([256, 256, 3, 3])\n",
      "layer3.3.bn2.weight : torch.Size([256])\n",
      "layer3.3.bn2.bias : torch.Size([256])\n",
      "layer3.3.conv3.weight : torch.Size([1024, 256, 1, 1])\n",
      "layer3.3.bn3.weight : torch.Size([1024])\n",
      "layer3.3.bn3.bias : torch.Size([1024])\n",
      "layer3.4.conv1.weight : torch.Size([256, 1024, 1, 1])\n",
      "layer3.4.bn1.weight : torch.Size([256])\n",
      "layer3.4.bn1.bias : torch.Size([256])\n",
      "layer3.4.conv2.weight : torch.Size([256, 256, 3, 3])\n",
      "layer3.4.bn2.weight : torch.Size([256])\n",
      "layer3.4.bn2.bias : torch.Size([256])\n",
      "layer3.4.conv3.weight : torch.Size([1024, 256, 1, 1])\n",
      "layer3.4.bn3.weight : torch.Size([1024])\n",
      "layer3.4.bn3.bias : torch.Size([1024])\n",
      "layer3.5.conv1.weight : torch.Size([256, 1024, 1, 1])\n",
      "layer3.5.bn1.weight : torch.Size([256])\n",
      "layer3.5.bn1.bias : torch.Size([256])\n",
      "layer3.5.conv2.weight : torch.Size([256, 256, 3, 3])\n",
      "layer3.5.bn2.weight : torch.Size([256])\n",
      "layer3.5.bn2.bias : torch.Size([256])\n",
      "layer3.5.conv3.weight : torch.Size([1024, 256, 1, 1])\n",
      "layer3.5.bn3.weight : torch.Size([1024])\n",
      "layer3.5.bn3.bias : torch.Size([1024])\n",
      "layer4.0.conv1.weight : torch.Size([512, 1024, 1, 1])\n",
      "layer4.0.bn1.weight : torch.Size([512])\n",
      "layer4.0.bn1.bias : torch.Size([512])\n",
      "layer4.0.conv2.weight : torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight : torch.Size([512])\n",
      "layer4.0.bn2.bias : torch.Size([512])\n",
      "layer4.0.conv3.weight : torch.Size([2048, 512, 1, 1])\n",
      "layer4.0.bn3.weight : torch.Size([2048])\n",
      "layer4.0.bn3.bias : torch.Size([2048])\n",
      "layer4.0.downsample.0.weight : torch.Size([2048, 1024, 1, 1])\n",
      "layer4.0.downsample.1.weight : torch.Size([2048])\n",
      "layer4.0.downsample.1.bias : torch.Size([2048])\n",
      "layer4.1.conv1.weight : torch.Size([512, 2048, 1, 1])\n",
      "layer4.1.bn1.weight : torch.Size([512])\n",
      "layer4.1.bn1.bias : torch.Size([512])\n",
      "layer4.1.conv2.weight : torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight : torch.Size([512])\n",
      "layer4.1.bn2.bias : torch.Size([512])\n",
      "layer4.1.conv3.weight : torch.Size([2048, 512, 1, 1])\n",
      "layer4.1.bn3.weight : torch.Size([2048])\n",
      "layer4.1.bn3.bias : torch.Size([2048])\n",
      "layer4.2.conv1.weight : torch.Size([512, 2048, 1, 1])\n",
      "layer4.2.bn1.weight : torch.Size([512])\n",
      "layer4.2.bn1.bias : torch.Size([512])\n",
      "layer4.2.conv2.weight : torch.Size([512, 512, 3, 3])\n",
      "layer4.2.bn2.weight : torch.Size([512])\n",
      "layer4.2.bn2.bias : torch.Size([512])\n",
      "layer4.2.conv3.weight : torch.Size([2048, 512, 1, 1])\n",
      "layer4.2.bn3.weight : torch.Size([2048])\n",
      "layer4.2.bn3.bias : torch.Size([2048])\n",
      "fc.weight : torch.Size([1000, 2048])\n",
      "fc.bias : torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "# from torchsummary import summary\n",
    "import torch\n",
    "from torchvision.models.resnet import resnet50\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = resnet50(pretrained=False, progress=True)   # 创建模型\n",
    "model.load_state_dict(torch.load(\"resnet50.pth\",map_location='cpu')) # 加载模型参数\n",
    "\n",
    "# 查看模型结构\n",
    "print(model)\n",
    "\n",
    "# 查看模型参数\n",
    "for name, parameters in model.named_parameters():\n",
    "    print(name, ':', parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbad94a-435d-40f4-8e86-8eabd9af1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf resnet50.onnx torchscript.pt\n",
    "\n",
    "model.eval()     # 设置模型为推理模式\n",
    "\n",
    "# 导出网络到ONNX，需提供输入输出名\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)  # 输入样本\n",
    "torch.onnx.export(\n",
    "    model,dummy_input,\"resnet50.onnx\",\n",
    "    opset_version=13,           # 转为onnx的版本\n",
    "    do_constant_folding=True,   # 是否执行常量折叠优化\n",
    "    input_names=[\"input_name\"],     # 输入名\n",
    "    output_names=[\"output_name\"],   # 输出名\n",
    "    # dynamic_axes={\n",
    "    #     \"input\":{0:\"batch_size\"},   # 批处理变量\n",
    "    #     \"output\":{0:\"batch_size\"}\n",
    "    # },\n",
    "    # dynamic_axes={'input_name': [2, 3], 'output_name': [2, 3]}   # 动态size的输入输出的维度\n",
    ")\n",
    "\n",
    "\n",
    "# 保存TORCHSCRIPT\n",
    "# torch script保存的模型，目前不提供输入和输出的端口的命名，\n",
    "# 因此在配置文件中，输入和输出端口的名字必须按照如下命名： \"INPUT__0\", \"INPUT__1\" and \"OUTPUT__0\", \"OUTPUT__1\"\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)  # 输入样本\n",
    "traced_cell = torch.jit.trace(model, dummy_input)\n",
    "traced_cell.save(\"resnet50-torchscript.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b72b53-407b-4efd-98ab-34d738a6da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置pytorch模型 config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd9529-7113-49a5-aa6c-c3f22a85340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: \"resnet50\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size : 0\n",
    "input [\n",
    "  {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "    reshape { shape: [ 1, 3, 224, 224 ] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "    reshape { shape: [ 1, 1000, 1, 1 ] }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fb5cc-5a0b-4501-8a2c-5123e5c181b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch 转为onnx 使用triton 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba522eab-ed7e-493d-8b95-25b2d3a0678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.tencent.com/pypi/simple/, https://mirrors.tencent.com/repository/pypi/tencent_pypi/simple\n",
      "Collecting netron\n",
      "  Downloading https://mirrors.tencent.com/pypi/packages/4c/2d/afa00587db942b710033a2362d7afff16b739af955c520a80ee21c8db28b/netron-5.9.4-py3-none-any.whl (1.5 MB)\n",
      "     |████████████████████████████████| 1.5 MB 811 kB/s            \n",
      "\u001b[?25hInstalling collected packages: netron\n",
      "Successfully installed netron-5.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Serving 'resnet50.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install onnx netron\n",
    "# 查看网络模型结构\n",
    "import netron\n",
    "modelPath = \"resnet50.onnx\"\n",
    "netron.start(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d146a93-5f3f-48bc-95a3-3316e0d6548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置onnx模型 config.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e289e8-0c1e-4990-8152-b885f05ff88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: \"resnet50\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "backend: \"onnxruntime\"\n",
    "max_batch_size : 0\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"data\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "    reshape { shape: [ 1, 3, 224, 224 ] }\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"resnetv17_dense0_fwd\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "    reshape { shape: [ 1, 1000 ] }\n",
    "  }\n",
    "]\n",
    "\n",
    "parameters { key: \"intra_op_thread_count\" value: { string_value: \"30\" } }\n",
    "parameters { key: \"execution_mode\" value: { string_value: \"1\" } }\n",
    "parameters { key: \"inter_op_thread_count\" value: { string_value: \"30\" } }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91df931-d44e-4835-a609-8d9c41878754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 客户端请求\n",
    "import datetime\n",
    "import argparse\n",
    "from functools import partial\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from attrdict import AttrDict\n",
    "import struct\n",
    "import tritonclient.grpc as grpcclient\n",
    "import tritonclient.grpc.model_config_pb2 as mc\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "from tritonclient.utils import triton_to_np_dtype\n",
    "from urllib.parse import quote, quote_plus\n",
    "import queue\n",
    "import json,time\n",
    "import pysnooper\n",
    "# pip3 install Pillow\n",
    "import requests\n",
    "\n",
    "# @pysnooper.snoop()\n",
    "def parse_model(model_metadata, model_config):\n",
    "    \"\"\"\n",
    "    Check the configuration of a model to make sure it meets the\n",
    "    requirements for an image classification network (as expected by\n",
    "    this client)\n",
    "    \"\"\"\n",
    "    if len(model_metadata.inputs) != 1:\n",
    "        raise Exception(\"expecting 1 input, got {}\".format(\n",
    "            len(model_metadata.inputs)))\n",
    "    if len(model_metadata.outputs) != 1:\n",
    "        raise Exception(\"expecting 1 output, got {}\".format(\n",
    "            len(model_metadata.outputs)))\n",
    "\n",
    "    if len(model_config.input) != 1:\n",
    "        raise Exception(\n",
    "            \"expecting 1 input in model configuration, got {}\".format(\n",
    "                len(model_config.input)))\n",
    "\n",
    "    input_metadata = model_metadata.inputs[0]\n",
    "    input_config = model_config.input[0]\n",
    "    output_metadata = model_metadata.outputs[0]\n",
    "\n",
    "    if output_metadata.datatype != \"FP32\":\n",
    "        raise Exception(\"expecting output datatype to be FP32, model '\" +\n",
    "                        model_metadata.name + \"' output type is \" +\n",
    "                        output_metadata.datatype)\n",
    "\n",
    "    # Output is expected to be a vector. But allow any number of\n",
    "    # dimensions as long as all but 1 is size 1 (e.g. { 10 }, { 1, 10\n",
    "    # }, { 10, 1, 1 } are all ok). Ignore the batch dimension if there\n",
    "    # is one.\n",
    "    output_batch_dim = (model_config.max_batch_size > 0)\n",
    "    non_one_cnt = 0\n",
    "    for dim in output_metadata.shape:\n",
    "        if output_batch_dim:\n",
    "            output_batch_dim = False\n",
    "        elif dim > 1:\n",
    "            non_one_cnt += 1\n",
    "            if non_one_cnt > 1:\n",
    "                raise Exception(\"expecting model output to be a vector\")\n",
    "\n",
    "    # Model input must have 3 dims, either CHW or HWC (not counting\n",
    "    # the batch dimension), either CHW or HWC\n",
    "    input_batch_dim = (model_config.max_batch_size > 0)\n",
    "    expected_input_dims = 3 + (1 if input_batch_dim else 0)\n",
    "    if len(input_metadata.shape) != expected_input_dims:\n",
    "        raise Exception(\n",
    "            \"expecting input to have {} dimensions, model '{}' input has {}\".\n",
    "                format(expected_input_dims, model_metadata.name,\n",
    "                       len(input_metadata.shape)))\n",
    "\n",
    "    if type(input_config.format) == str:\n",
    "        FORMAT_ENUM_TO_INT = dict(mc.ModelInput.Format.items())\n",
    "        input_config.format = FORMAT_ENUM_TO_INT[input_config.format]\n",
    "\n",
    "    if ((input_config.format != mc.ModelInput.FORMAT_NCHW) and\n",
    "            (input_config.format != mc.ModelInput.FORMAT_NHWC)):\n",
    "        raise Exception(\"unexpected input format \" +\n",
    "                        mc.ModelInput.Format.Name(input_config.format) +\n",
    "                        \", expecting \" +\n",
    "                        mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NCHW) +\n",
    "                        \" or \" +\n",
    "                        mc.ModelInput.Format.Name(mc.ModelInput.FORMAT_NHWC))\n",
    "\n",
    "    if input_config.format == mc.ModelInput.FORMAT_NHWC:\n",
    "        h = input_metadata.shape[1 if input_batch_dim else 0]\n",
    "        w = input_metadata.shape[2 if input_batch_dim else 1]\n",
    "        c = input_metadata.shape[3 if input_batch_dim else 2]\n",
    "    else:\n",
    "        c = input_metadata.shape[1 if input_batch_dim else 0]\n",
    "        h = input_metadata.shape[2 if input_batch_dim else 1]\n",
    "        w = input_metadata.shape[3 if input_batch_dim else 2]\n",
    "\n",
    "    return (model_config.max_batch_size, input_metadata.name,\n",
    "            output_metadata.name, c, h, w, input_config.format,\n",
    "            input_metadata.datatype)\n",
    "\n",
    "\n",
    "def preprocess(img, format, dtype, c, h, w, scaling, protocol):\n",
    "    \"\"\"\n",
    "    Pre-process an image to meet the size, type and format\n",
    "    requirements specified by the parameters.\n",
    "    \"\"\"\n",
    "    # np.set_printoptions(threshold='nan')\n",
    "\n",
    "    if c == 1:\n",
    "        sample_img = img.convert('L')\n",
    "    else:\n",
    "        sample_img = img.convert('RGB')\n",
    "\n",
    "    resized_img = sample_img.resize((w, h), Image.BILINEAR)\n",
    "    resized = np.array(resized_img)\n",
    "    if resized.ndim == 2:\n",
    "        resized = resized[:, :, np.newaxis]\n",
    "\n",
    "    npdtype = triton_to_np_dtype(dtype)\n",
    "    typed = resized.astype(npdtype)\n",
    "\n",
    "    if scaling == 'INCEPTION':\n",
    "        scaled = (typed / 127.5) - 1\n",
    "    elif scaling == 'VGG':\n",
    "        if c == 1:\n",
    "            scaled = typed - np.asarray((128,), dtype=npdtype)\n",
    "        else:\n",
    "            scaled = typed - np.asarray((123, 117, 104), dtype=npdtype)\n",
    "    else:\n",
    "        scaled = typed\n",
    "\n",
    "    # Swap to CHW if necessary\n",
    "    if format == mc.ModelInput.FORMAT_NCHW:\n",
    "        ordered = np.transpose(scaled, (2, 0, 1))\n",
    "    else:\n",
    "        ordered = scaled\n",
    "\n",
    "    # Channels are in RGB order. Currently model configuration data\n",
    "    # doesn't provide any information as to other channel orderings\n",
    "    # (like BGR) so we just assume RGB.\n",
    "    return ordered\n",
    "\n",
    "\n",
    "def convert_http_metadata_config(_metadata, _config):\n",
    "    _model_metadata = AttrDict(_metadata)\n",
    "    _model_config = AttrDict(_config)\n",
    "\n",
    "    return _model_metadata, _model_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name='resnet50'\n",
    "model_version='1'\n",
    "url = 'resnet50-tensorrt.service.kfserving.woa.com'\n",
    "classes=1000\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    triton_client = httpclient.InferenceServerClient(url=url, verbose=False, concurrency=1)\n",
    "\n",
    "    # 获取模型的配置信息\n",
    "    model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
    "    # print(model_metadata)\n",
    "    model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)\n",
    "    # print(model_config)\n",
    "    model_metadata, model_config = convert_http_metadata_config(model_metadata, model_config)\n",
    "    print(model_metadata,model_config)\n",
    "    max_batch_size, input_name, output_name, c, h, w, format, dtype = parse_model(model_metadata, model_config)\n",
    "    # max_batch_size, input_name, output_name, c, h, w, format, dtype = 0,'data','resnetv17_dense0_fwd',3,224,224,2,'FP32'\n",
    "    print(max_batch_size, input_name, output_name, c, h, w, format, dtype)\n",
    "\n",
    "    filename = 'smallcat.jpg'\n",
    "\n",
    "    image_data = preprocess(Image.open(filename), format, dtype, c, h, w, None,'http')\n",
    "    batched_image_data = image_data\n",
    "\n",
    "    # Send request\n",
    "    input = httpclient.InferInput(input_name, batched_image_data.shape, dtype)\n",
    "    input.set_data_from_numpy(batched_image_data)\n",
    "    output = httpclient.InferRequestedOutput(output_name, class_count=classes)\n",
    "\n",
    "\n",
    "    response = triton_client.infer(model_name,[input],request_id=str(time.time()),model_version=model_version,outputs=[output])\n",
    "    output_array = response.as_numpy(output_name)\n",
    "    print(output_array)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
